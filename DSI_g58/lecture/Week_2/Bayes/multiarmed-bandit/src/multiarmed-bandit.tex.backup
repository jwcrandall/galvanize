%% beamer packages
% other themes: AnnArbor, Antibes, Bergen, Berkeley, Berlin, Boadilla, boxes, 
% CambridgeUS, Darmstadt, Dresden, Frankfurt, Goettingen, Hannover, Ilmenau,
%JuanLesPins, Luebeck, Madrid, Malmoe, Marburg, Montpellier, PaloAlto,
%Pittsburgh, Rochester, Singapore, Szeged, Warsaw
% other colors: albatross, beaver, crane, default, dolphin, dove, fly, lily, 
%orchid, rose, seagull, seahorse, sidebartab, structure, whale, wolverine,
%beetle

%\documentclass[xcolor=dvipsnames]{beamer}
\documentclass[table,dvipsnames]{beamer}
\usepackage{beamerthemesplit}
\usepackage{bm,amsmath,marvosym}
\usepackage{listings,color}%xcolor
\usepackage[ngerman]{babel}
\usepackage{natbib}
\usepackage{media9}
\usepackage[utf8]{inputenc}
\definecolor{shadecolor}{rgb}{.9, .9, .9}
\definecolor{darkblue}{rgb}{0.0,0.0,0.5}
\definecolor{myorange}{cmyk}{0,0.7,1,0}
\definecolor{mypurple}{cmyk}{0.3, 0.9, 0.0, 0.2}

% make a checkmark
\usepackage{tikz}
\def\checkmark{\tikz\fill[scale=0.4](0,.35) -- (.25,0) -- (1,.7) -- (.25,.15) -- cycle;} 

% dot product
\usetikzlibrary{arrows,positioning}
\tikzset{
    %Define standard arrow tip
    >=stealth',
    % Define arrow style
    pil/.style={->,thick}
}

% math stuff
\newcommand{\argmin}{\operatornamewithlimits{argmin}}

\lstnewenvironment{code}{
    \lstset{backgroundcolor=\color{shadecolor},
        showstringspaces=false,
        language=python,
        frame=single,
        framerule=0pt,
        keepspaces=true,
        breaklines=true,
        basicstyle=\ttfamily,
        keywordstyle=\bfseries,
        basicstyle=\ttfamily\scriptsize,
        keywordstyle=\color{blue}\ttfamily,
        stringstyle=\color{red}\ttfamily,
        commentstyle=\color{green}\ttfamily,
        columns=fullflexible
    }
}{}

\lstnewenvironment{codeout}{
    \lstset{backgroundcolor=\color{shadecolor},
        frame=single,
        framerule=0pt,
        breaklines=true,
        basicstyle=\ttfamily\scriptsize,
        columns=fullflexible
    }
}{}

\hypersetup{colorlinks = true, linkcolor=darkblue, citecolor=darkblue,urlcolor=darkblue}
\hypersetup{pdfauthor={A. Richards}, pdftitle={Multi-armed Bandit}}

\newcommand{\rd}{\textcolor{red}}
\newcommand{\grn}{\textcolor{green}}
\newcommand{\keywd}{\textcolor{myorange}}
\newcommand{\highlt}{\textcolor{NavyBlue}}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\def\ci{\perp\!\!\!\perp}
% set beamer theme and color
\usetheme{Frankfurt}
%\usetheme{Berkeley}
\usecolortheme{orchid}
%\usecolortheme{seagull}


%% fix the section title for literature
\renewcommand{\bibsection}{\subsubsection*{\bibname } }

\title[Bayesian A/B]{Multi-armed Bandit}
\author[A. Richards]{A. Richards}
\institute{}
\date[]{02.08.2017}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}
\frame{\titlepage}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\frame{
\footnotesize
\tableofcontents
\normalsize
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{A/B frameworks}
\subsection{}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\frame{   
\frametitle{Objectives}
\begin{block}{Morning}
 \begin{itemize}
  \item Overview of Frequentist A/B testing
  \item Overview of Bayesian A/B testing
  \item Review of Bayes’ Theorem 
  \item Conjugate Priors
  \item Is CTR$_{A}$ is better than CTR$_B$ through Code
  \end{itemize}
\end{block}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\frame{   
\frametitle{Frequentist A/B testing}
\footnotesize

\begin{enumerate}
 \item define a metric
 \item determine parameters of interest for study (number of observations, 
power, significance threshold, and so on)
 \item run test, without checking results, until number of observations has
been achieved
 \item calculate p-value associated with hypothesis test
 \item report $p$-value and suggestion for action
\end{enumerate}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\frame{   
\frametitle{Bayesian A/B testing}
\footnotesize
\begin{enumerate}
 \item Define a metric
 \item Run test, continually monitor results
 \item At any time calculate probability that $A \geq B$ or vice versa
 \item Suggest course of action based on probabilities calculated
\end{enumerate}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\frame{   
\frametitle{Discussion}

Obtaining significance depends on \highlt{power}

\begin{block}{}
 \begin{itemize}
  \item What are the factors that influence power?
  \item Which A/B testing framework relies on significance testing?
  \item What about the other framework what does it use to guide decisions?
  \end{itemize}
\end{block}

With the Bayesian framework you:
\begin{itemize}
 \item have a degree of Belief?
 \item can say \textit{it is 95\% likely that site A is better than site B}?
 \item can stop a test early based on surprising data
\end{itemize}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\frame{ 
\frametitle{That formula again}
\footnotesize
\begin{equation}
P(\theta|y) = \frac{P(y|\theta)P(\theta)}{P(y)}
\end{equation}

\begin{itemize}
 \item \keywd{prior} - $P(\theta)$ - one's beliefs about a quantity before 
presented with evidence
 \item \keywd{posterior} - $P(\theta|y)$ - probability of the parameters given 
the evidence
 \item \keywd{likelihood} - $P(y|\theta)$  - probability of the evidence given 
the parameters
 \item \keywd{normalizing constant} - $P(y)$
\end{itemize}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Priors}
\subsection{}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]
\frametitle{Lets talk about priors}
\footnotesize

\begin{block}{Subjective vs Objective priors}
Bayesian priors can be classified into two classes: \keywd{objective priors}, 
which aim to allow the data to influence the posterior the most, and 
\keywd{subjective priors}, which allow the practitioner to express his or her 
views into the prior.
\end{block}

\begin{itemize}
 \item If we added more probability mass to certain areas of 
the prior, and less elsewhere, we are biasing our inference towards the 
unknowns 
existing in the former area.
\item The prior's influence changes as our dataset increases
\end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\frame{
\begin{block}{Empirical Bayes}
It is not a true Bayesian method. Empirical Bayes combines frequentist and 
Bayesian inference.  The prior distribution, instead of being selected 
beforehand is estimated directly from the data generally with frequentist 
methods.
\end{block}
\vspace{0.5cm}
\href{https://en.wikipedia.org/wiki/Empirical\_Bayes\_method}{
https://en.wikipedia.org/wiki/Empirical\_Bayes\_method}
\vspace{0.5cm}
\begin{block}{CAUTION}
 Many people feel that empirical bayes is \textit{double counting} or 
\textit{double dipping} from the data
\end{block}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\frame{
\frametitle{The Gamma distribution}
\scriptsize
A Gamma random variable, denoted $X \sim \text{Gamma}(\alpha, \beta)$, is 
also

\begin{equation}
\text{Exp}(\beta) \sim \text{Gamma}(1, \beta)
\end{equation}

The additional parameter gives flexibility which helps us better express
subjective priors. The density function for a $\text{Gamma}(\alpha, \beta)$ 
random variable is:

\begin{equation}
f(x \mid \alpha, \beta) = \frac{\beta^{\alpha}x^{\alpha-1}e^{-\beta 
x}}{\Gamma(\alpha)} 
\end{equation}

where $\Gamma(\alpha)$ is the 
\href{http://en.wikipedia.org/wiki/Gamma\_function)}{Gamma 
function}

\begin{center}
\includegraphics[scale=0.32]{gamma-priors.png}
\end{center}

\begin{flushleft}
 
\tiny{\href{
http://nbviewer.jupyter.org/github/CamDavidsonPilon/Probabilistic-Programming-an
d-Bayesian-Methods-for-Hackers/blob/master/Chapter6\_Priorities/Ch6\_Priors\_PyM
C 2 . ipynb}{Bayesian methods for hackers (chap 6)}}
\end{flushleft}

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\frame{
\frametitle{The Beta distribution}
\footnotesize

The Beta distribution is very useful in Bayesian statistics. A random variable 
$X$ has a $\text{Beta}$ distribution, with parameters $(\alpha, \beta)$, if its 
density function is:

\begin{equation}
f_X(x | \; \alpha, \beta ) = \frac{ x^{(\alpha - 1)}(1-x)^{ (\beta - 1) } 
}{B(\alpha, \beta) }
\end{equation}

where $B$ is the \href{http://en.wikipedia.org/wiki/Beta\_function}{Beta 
function} (hence the name). The random variable $X$ is only allowed in [0,1], 
making the Beta distribution a popular distribution for decimal values, 
probabilities and proportions. The values of $\alpha$ and $\beta$, both positive 
values, provide great flexibility in the shape of the distribution.

\begin{center}
\includegraphics[scale=0.32]{beta-priors.png}
\end{center}

% \begin{flushleft}
% \tiny{\href{
% http://nbviewer.jupyter.org/github/CamDavidsonPilon/Probabilistic-Programming-an
% d-Bayesian-Methods-for-Hackers/blob/master/Chapter6\_Priorities/Ch6\_Priors\_PyM
% C 2.ipynb}{Bayesian methods for hackers (chap 6)}}
% \end{flushleft}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\frame{
\frametitle{So what?}
\footnotesize
There is an interesting connection between the Beta distribution and the 
Binomial distribution. Suppose we are interested in some unknown proportion or 
probability $p$. (think of coin example)
\vspace{0.5cm}
\begin{itemize}
 \item We assign a $\text{Beta}(\alpha, \beta)$ prior to $p$. We observe some 
data generated by a Binomial process, say $X \sim \text{Binomial}(N, p)$, with 
$p$ still unknown. 

\item Then our posterior \textit{is again a Beta distribution}, i.e. $p | X 
\sim \text{Beta}( \alpha + X, \beta + N -X )$. 

\item If we start with a $\text{Beta}(1,1)$ prior on $p$ (which is a Uniform), 
observe data $X \sim \text{Binomial}(N, p)$, then our posterior is 
$\text{Beta}(1 + X, 1 + N - X)$. 
\end{itemize}
\vspace{0.5cm}
\highlt{A Beta prior with Binomial observations creates a Beta 
posterior}. This is a very useful property, both computationally and 
heuristically.
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\frame{
\frametitle{Beta conjugate to the binomial proof}
If you are curious how we can identify a conjugate prior you can follow the proof (5 minutes).
\vspace{1cm}
\begin{center}
 \href{https://www.youtube.com/watch?v=hKYvZF9wXkk}{https://www.youtube.com/watch?v=hKYvZF9wXkk}
\end{center}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\frame{   
\frametitle{Conjugate priors}
\footnotesize

Recall that

\begin{equation}
P(y|\theta) \propto p(\theta|y) p(\theta)
\end{equation}

\begin{itemize}
 \item Conjugate families of priors arise when the likelihood times the prior 
produces a recognizable posterior kernel
 \item For mathematical convenience, we construct a family of prior densities 
that lead to simple posterior densities.
 \item Conjugate prior distributions have the practical advantage, in addition 
to computational
convenience, of being interpretable as additional data
 \item Probability distributions that belong to an 
\href{https://en.wikipedia.org/wiki/Exponential_family}{exponential family} 
have natural conjugate prior distributions
\end{itemize}
}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]
\frametitle{Is $CTR_{A}$ better than $CTR_{B}$?}
\begin{code}
import numpy as np

num_samples = 10000
A = np.random.beta(1 + num_clicks_A,
1 + num_views_A - num_clicks_A,
size=num_samples)
B = np.random.beta(1 + num_clicks_B,
1 + num_views_B - num_clicks_B,
size=num_samples)
# Probability that A wins:
print(np.sum(A > B) / float(num_samples))
\end{code}

\end{frame}


\frame{ 
\frametitle{Check-in questions}
\begin{block}{}
\highlt{Core questions}
\begin{enumerate} 
 \item Why might we consider a Bayesian approach over a frequentist in an A/B testing scenario?
 \item What is a conjugate prior? (Can you provide an example)
 \end{enumerate}

\highlt{Bonus questions}
\begin{enumerate} 
 \item What is meant by empirical Bayes?
 \item How is the Gamma related to the Exponential distribution?
 \item What are the limits on $\alpha$ and $\beta$ in the Beta distribution?
\end{enumerate} 
\end{block}
}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Multiarmed bandit}
\subsection{}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\frame{   
\frametitle{Objectives}
\scriptsize
\begin{block}{Morning}
 \begin{itemize}
  \item [\checkmark] Overview of Frequentist A/B testing
  \item [\checkmark] Overview of Bayesian A/B testing
  \item [\checkmark] Review of Bayes’ Theorem 
  \item [\checkmark] Conjugate Priors
  \item [\checkmark] Determining whether CTR$_{A}$ is better than CTR$_B$ 
through Code
  \end{itemize}
\end{block}

\begin{block}{Afternoon}
 \begin{itemize}
 \item Introduction and use cases of Multi-Arm Bandits
 \item Zen and the Art of Minimizing Regret
 \item Overview of Common Strategies
\begin{itemize}
\scriptsize
 \item Epsilon-Greedy
 \item Softmax
 \item UCB1
 \item Bayesian Bandit
 \end{itemize} 
\item Other forms of Multi-Arm Bandits
\end{itemize}
\end{block}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\frame{
\frametitle{What's up with the name?}
\scriptsize
\begin{block}{Multi-armed bandit}
Suppose you are faced with $N$ slot machines. Each bandit has an unknown 
probability of distributing a prize (assume for now the prizes are the same for 
each bandit, only the probabilities differ). Some bandits are very generous, 
others not so much. Of course, you don't know what these probabilities are. By 
only choosing one bandit per round, our task is devise a strategy to maximize 
our winnings.
\end{block}
\begin{center}
\includegraphics[scale=0.2]{multiarmed-bandit.jpg}
\end{center}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\frame{
\frametitle{The bandits problem}
\scriptsize
\begin{itemize}
 \item If we knew the bandit with the largest probability, then always picking 
this bandit would yield the maximum winnings. So our task can be phrased as 
\highlt{Find the best bandit, and as quickly as possible}. 

\item The task is complicated by the stochastic nature of the bandits. A 
suboptimal bandit can return many winnings, purely by chance, which would make 
us believe that it is a very profitable bandit. Similarly, the best bandit can 
return many duds. Should we keep trying losers then, or give up? 

\item A more troublesome problem is, if we have found a bandit that returns 
\textit{pretty good} results, do we keep drawing from it to maintain our 
\textit{pretty good score}, or do we try other bandits in hopes of finding an 
\textit{even-better} bandit? This is the \highlt{exploration} vs. 
\highlt{exploitation} dilemma.
\end{itemize}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\frame{ 
\frametitle{Exploration vs Exploitation}
\begin{itemize}
\item \keywd{Exploration}: Trying out different options to try and determine
the reward associated with the given approach (i.e. acquiring
more knowledge)
\ \\ \
\item \keywd{Exploitation}: Going with the approach that you believe to
have the highest expected payoff (i.e. optimizing decisions
based on existing knowledge)
\end{itemize}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\frame{
\frametitle{Multi-armed bandit applications go beyond A/B}
\footnotesize
\begin{itemize}
 \item \highlt{Internet display advertising}: What ad strategy will maximize 
sales? Naturally minimizing strategies that do not work (generalizes to A/B/C/D 
strategies)
 \item \highlt{Ecology}: How do the animals maximize its fitness w.r.t energy?
  \item \highlt{Finance}: which stock option gives the highest return, under 
time-varying return profiles.
  \item \highlt{Clinical trials}: a researcher would like to find the best 
treatment, out of many possible treatment, while minimizing losses. 
  \item \highlt{Psychology}: How does punishment and reward affect our 
behaviour? How do humans learn?
\end{itemize}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\frame{ 
\frametitle{Traditional A/B testing}

The process
\begin{itemize}
\item Start with pure exploration in which groups A and B are
assigned equal number of users
\item Once you think you have determined the better option, switch
to pure exploitation in which you stop the experiment and send
all users to the better performer
\end{itemize}

Potential issues
\begin{itemize}
 \item Equal number of observations are routed to A and B for a
preset amount of time or iterations
 \item Only after that preset amount of time or iterations do we stop
and use the better performer
 \item Waste time (and money!) showing users the site that isn’t
performing as well
\end{itemize}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\frame{
\frametitle{Multi-armed bandit solutions}
\begin{itemize}
\item Shows a user the site that you think is best most of the time
(exactly how is dictated by the strategy chosen)
\item As the experiment runs, we update the belief about the true
CTR (Click Through Rate)
\item Run for however long until we are satisfied the experiment has
determined the better site
\item Balances exploration and exploitation rather than doing only
one or the other
\end{itemize}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\frame{
\frametitle{formalization}
\begin{itemize}
\item The model is given by a set of real distributions $B = {R_{1}, ..., 
R_{K}}$,
\item where each distribution is associated with the a reward delivered by
one of the $K \in N$ + levers.
\item We will let $\mu_{1}, ... , \mu_{K}$ be the mean values associated with 
these reward distributions.
\item The gambler plays one lever per round and observes the associated
reward. 
\item The goal is to maximize the sum of the collective rewards,
or alternatively minimize the agent’s \keywd{regret}
\end{itemize}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\frame{
\frametitle{Regret}
\begin{block}{}
The regret $p$ that an agent experiences after $T$ rounds is the
difference between the reward sum associated with an optimal
strategy and the sum of collected rewards
\end{block}

\begin{equation}
p = T \mu^{*} - \sum^{T}_{t=1} \hat{r}_{t}
\end{equation}

\begin{itemize}
 \item $\mu^{*}$ - is the maximal reward mean, $\mu^{*} = max_{k}\{\mu_{k}\}$
 \item $\hat{r}_{t}$ - is the reward at time t
 \item \keywd{Regret} is simply a measure of how often you choose a suboptimal 
bandit. We can think of this as the cost function we are trying to minimize
\end{itemize}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\frame{
\frametitle{A zero-regret strategy}

\begin{itemize}
 \item A zero-regret strategy is a strategy whose average regret per
round p/T tends to zero when the number of rounds played
tends toward infinity
 \item Interestingly enough, a zero-regret strategy does not guarantee
you will never choose a sub-optimal outcome, but rather
guarantees that, over time, you will tend to choose the optimal
outcome
\end{itemize}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\frame{
\frametitle{Epsilon-Greedy strategy}
\begin{itemize}
 \item Explore with some probability $epsilon$ (often 10\%)
 \item All other times we will exploit (i.e. choose the bandit with the
best performance so far)
 \item After we choose a given bandit we update the performance
based on the result.
\end{itemize}
}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\frame{
\frametitle{Other strategies}
\begin{block}{UCB1 - Upper confidence bound}
For the UCB1 algorithm we will choose whichever bandit that has
the largest value. 
\end{block}
\vspace{1cm}
\begin{block}{softmax}
For the softmax algorithm we will choose the bandit randomly in
proportion to its estimated value.
\end{block}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Bayesian Bandits}
\subsection{}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\frame{
\frametitle{Bayesian Bandits}
The Bayesian bandit algorithm involves modeling each of our bandits
with a beta distribution with the following shape parameters:
\begin{itemize}
 \item $\alpha$ = 1 + number of times bandit has won
 \item $\beta$ = 1 + number of times bandit has lost
\end{itemize}

We will then take a random sample from each bandit’s distribution
and choose the bandit with the highest value.
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\frame{
\frametitle{Bayesian Bandits}
\scriptsize
\begin{block}{} 
There are also many approximately-optimal solutions which are quite 
good. One of the few solutions that can scale incredibly well. The solution is 
known as \keywd{Bayesian Bandits}.
\end{block}
\vspace{1cm}
These strategies are an example of \keywd{online algorithms} because they are 
continuously-being-updated, aka \keywd{reinforcement 
learning algorithm}. The algorithm starts in an ignorant state, where it knows 
nothing, and begins to acquire data by testing the system. As it acquires data 
and results, it learns what the best and worst behaviors are (in this case, it 
learns which bandit is the best).
}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\frame{
\begin{itemize}
 \item Like in a normal multi-arm problem, an agent must choose
between arms during each iteration
 \item Before making the choice, the agent sees a d-dimensional
feature vector (context vector), associated with the current
iterations state
 \item The agent uses the context vector as well as the history of past
rewards to choose the arm to play in the current iteration
 \item Over time, the aim is for the agent to learn how the context
vectors relate to the associated rewards so as to pick the
optimal arm
\end{itemize}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\frame{
\frametitle{Summary}
\begin{itemize}
\item The problem originated with a gambler standing in front of a
row of slot machines (referred to as \textit{one-armed bandits})
\item The agent has to decide which machines to play, how many
times to play each machine, and it which order
\item Each bandit will provide a reward from a unknown distribution
\item Objective is to maximize the sum of rewards earned through a
series of lever pulls
\item There are several classes of fairly optimal solutions 
\end{itemize}
}















% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \frame{
% \frametitle{Markov chain Monte Carlo (MCMC)}
% \footnotesize
% \keywd{MCMC}
% \begin{itemize}
%  \item It is an family of algorithms for obtaining a sequence of random samples from a probability distribution for which direct sampling is difficult.
%  \item The sequence can then be used to approximate the distribution
%  \item It allows for inference on complex models
% \end{itemize}
% 
% A particularly useful class of MCMC, known as Hamliltonian Monte Carlo, requires \href{https://en.wikipedia.org/wiki/Gradient}{gradient} information which is often not readily available so PyMC3 uses Theano to get around this problem.  Something that has recently made this whole field a lot more interesting is the No-U-turn sampler (NUTS) because there are \highlt{self-tuning strategies} \citep{Hoffman14}.
% \\ \ \\
% One of the really nice things about probabilistic programming is that \highlt{you do not have to know how inference is performed}, but it can be useful.  
% 
% \begin{itemize}
%  \item \href{http://twiecki.github.io/blog/2015/11/10/mcmc-sampling/}{MCMC for Dummies}
%  \item \href{https://arxiv.org/pdf/1206.1901.pdf}{More on Hamliltonian MCMC (Not for dummies)}
%  \item \href{http://twiecki.github.io/blog/2014/01/02/visualizing-mcmc/}{How to animate MCMC (for everyone)}
% \end{itemize}
% }
% 
% 
% 
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}[fragile]
% \frametitle{PyMC3}
% \footnotesize
% \begin{code}
% import pymc3 as pm
% 
% n,h,alpha,beta,niter = 100,61,2,2,1000
% 
% # context management
% with pm.Model() as model: 
%     p = pm.Beta('p', alpha=alpha, beta=beta)
%     y = pm.Binomial('y', n=n, p=p, observed=h)
% 
%     start = pm.find_MAP()
%     step = pm.Metropolis()
%     trace = pm.sample(niter, step, start)
% \end{code}
% 
% Data $\rightarrow$ Model context $\rightarrow$ Priors $\rightarrow$ Likelihood $\rightarrow$ Sampler $\rightarrow$ Inference
% \vspace{0.5cm}
% \\ \noindent To the notebooks!
% \end{frame}
% 
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \frame{   
% \frametitle{Where are we...}
% \begin{block}{}
%  \begin{itemize}
%   \item[\checkmark] Overview
%   \item[\checkmark] Coin-flip example
%   \item[\checkmark] Estimating the mean and standard deviation of a Normal
%   \item[\checkmark] Switchpoint analysis of text messages
%   \end{itemize}
% \end{block}
% }
% 
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}[fragile]
% \frametitle{There are more examples..}
% \begin{block}{}
%  But what about linear regression--the classical example??
% \end{block}
% 
% The linear regression example \href{http://pymc-devs.github.io/pymc3/notebooks/getting_started.html}{is well explained in the docs}, but I did want to point out that we now have access to GLM style formulations in PyMC3.
% \vspace{1cm}
% \begin{code}
% import pymc3 as pm
% 
% ...
% 
% data = dict(x=x, y=y)
% 
% with pm.Model() as model:
%     pm.glm.glm('y ~ x', data)
%     step = pm.NUTS()
%     trace = pm.sample(2000, step, progressbar=True)
% \end{code}    
% \end{frame}
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \section{PMF}
% \subsection{}
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \frame{ 
% \frametitle{Recommenders}
% 
% \begin{table}
% \begin{center}
% What will a user \textit{buy}, \textit{click}, \textit{like}...
% \end{center}
% \ \\ \ \\
% \begin{tabular}{|l|c|c|c|c|c|c|c|c|}
% \hline
%         & A  &  B & C & D & E & F & G & ... \\
% \hline        
% Frodo   & 1  &  ? & 2 & 1 & 1 & 4 & 1 & ... \\
% Sam     & ?  &  ? & 1 & 3 & ? & 3 & 1 & ... \\
% Merry   & ?  &  ? & 1 & 1 & ? & 1 & 1 & ... \\
% Gimli   & 1  &  1 & ? & 1 & ? & 1 & ? & ... \\
% Legolas & 1  &  1 & ? & 1 & ? & 1 & ? & ... \\
% \hline
% \end{tabular}
% \end{table}
% 
% \begin{itemize}
%  \item Ebay and Amazon who recommends items for purchase
%  \item Movies, dating services, social network feeds, recipes, jokes, hikes, ...
% \end{itemize}
% }
% 
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \frame{ 
% \footnotesize
% 
% There are numerous types of recommender systems
% \begin{itemize}
%  \item \keywd{Popularity} - Most viewed, most well-liked, not customized to users
%  \item \keywd{User-User} - Recommend items liked by users with similar results
%  \item \keywd{Item-Item} - Recommend items similar to what the current user rated favorably
%  \item \keywd{Matrix-Factorization} - Estimate a users underlying preferences
% \end{itemize}
% -------------------------------------------------------------------------------- \\
% Two of the most commonly implemented varieties are:
% \begin{itemize}
%  \item \keywd{collaborative filtering} - Imagine that user 1 and user 2 both like the same 4 science fiction novels we can then infer that if user 1 also likes another similar novel then user 2 will have a good chance of also liking it.
%  \item \keywd{Content-based recommender} - These systems also make use of extra features associated with the user
% \end{itemize}
% }
% 
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \frame{ 
% \frametitle{Probabilistic matrix factorization (PMF)}
% \footnotesize
% \begin{itemize}
%  \item Probabilistic approach to the collaborative filtering problem that takes a Bayesian perspective \citep{Salakhutdinov08}.
%  \item The ratings $R$ are modeled as draws from a Gaussian distribution
%  \item We use precision $\alpha$, a fixed parameter, that reflects the uncertainty of the estimations; the normal distribution is commonly reparameterized in terms of precision, which is the inverse of the variance.
%  \item small precision parameters help control the growth of our latent parameters
%  \end{itemize}
% The following implementation is modified from the \href{https://pymc-devs.github.io/pymc3/notebooks/pmf-pymc.html}{example in the PyMC3 documentation}.
% \vspace{1cm}
% Back to the notebook!
% }
% 
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \frame{ 
% \frametitle{Conclusions}
% \footnotesize
% \begin{itemize}
%  \item Our results demonstrate that the mean of means method is our best baseline on our prediction task. 
%  \item We are able to obtain a significant decrease in RMSE using the PMF MAP estimate obtained via Powell optimization. 
%  \item We illustrated one way to monitor convergence of an MCMC sampler with a high-dimensionality sampling space 
%  \item The traceplots using this method seem to indicate that our sampler converged to the posterior.
%  \item Results using this posterior showed that attempting to improve the MAP estimation using MCMC sampling actually overfit the training data and increased test RMSE. This was likely caused by the constraining of the posterior via fixed precision parameters $\alpha$, $\alpha U$ and $\alpha V$.
% \end{itemize}
% 
% \href{https://gist.github.com/macks22/00a17b1d374dfc267a9a}{this gist} is a working version of a fully Bayesian implementation.
% }
% 
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \section{Cool down}
% \subsection{}
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \frame{ 
% \frametitle{Neural Nets}
% \begin{block}{Thomas Wiecki}
% Has a \href{http://twiecki.github.io/blog/2016/06/01/bayesian-deep-learning/}{great blog post talking about exactly how to do this}
% \end{block}
% 
% The notebook that is provided in this repo has not been significantly modified from  \href{https://github.com/twiecki/WhileMyMCMCGentlySamples/blob/master/content/downloads/notebooks/bayesian_neural_network.ipynb}{its original form}.  Although, check the original repository for the latest version.
% }
% 
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \frame{
% \begin{center}
% \includegraphics[scale=0.5]{../notebooks/nn-0.png}
% \end{center}
% }
% 
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \frame{
% \begin{center}
% \includegraphics[scale=0.5]{../notebooks/nn-1.png}
% \end{center}
% }
% 
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \frame{
% \begin{center}
% \includegraphics[scale=0.5]{../notebooks/nn-2.png}
% \end{center}
% }
% 
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \frame{ 
% \frametitle{Another take on probabilistic programming}
% \footnotesize
% \begin{block}{}
%  Another way of thinking about this: unlike a traditional program, which only runs in the forward directions, \highlt{a probabilistic program is run in both the forward and backward direction}. It runs forward to compute the consequences of the assumptions it contains about the world (i.e., the model space it represents), but it also runs backward from the data to constrain the possible explanations. In practice, many probabilistic programming systems will cleverly interleave these forward and backward operations to efficiently home in on the best explanations.
% \end{block}
% \vspace{1cm}
% \href{https://plus.google.com/u/0/107971134877020469960/posts/KpeRdJKR6Z1}{Why Probabilistic Programming matters? (Beau Cronin)}
% }
% 
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \frame{   
% \frametitle{What did we cover again?}
% \begin{block}{}
%  \begin{itemize}
%   \item[\checkmark] Overview
%   \item[\checkmark] Coin-flip example
%   \item[\checkmark] Estimating the mean and standard deviation of a Normal
%   \item[\checkmark] Switchpoint analysis of text messages
%   \item[\checkmark] Probabilistic matrix factorization
%   \item[\checkmark] Probabilistic neural networks
%   \end{itemize}
% \end{block}
% }
% 
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \frame{ 
% \frametitle{Where to go from here}
% \footnotesize
% 
% Examples, examples, examples...
% \begin{itemize}
%  \item \href{https://github.com/pymc-devs/pymc3}{PyMC3 repo}
%  \item \href{http://pymc-devs.github.io/pymc3/notebooks/getting_started.html}{Getting started guide}
%  \item \href{https://github.com/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers}{Bayesian methods 
% for hackers}
%   \item \href{http://twiecki.github.io}{Blog by Thomas Wiecki}
%   \item \href{http://www.amazon.com/Doing-Bayesian-Analysis-Second-Edition/dp/0124058884/ref=dp_ob_title_bk}{Doing Bayesian Data Analysis by John Kruschke}
%   \item \href{https://github.com/markdregan/Bayesian-Modelling-in-Python}{Resource by Mark Dregan}
% \end{itemize}
% 
% There is also \href{https://github.com/stan-dev/example-models/wiki}{PyStan} (\href{http://www.stat.columbia.edu/~gelman/research/unpublished/stan-resubmit-JSS1293.pdf}{Stan paper})
% }

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\frame[allowframebreaks]{  
\frametitle{References}
\begin{tiny} \bibliography{pp.bib}
\bibliographystyle{apalike}         % Style BST file
\end{tiny}
}

\end{document}