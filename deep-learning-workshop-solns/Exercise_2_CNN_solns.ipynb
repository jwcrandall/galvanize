{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2: CNN to classify Cifar10 dataset\n",
    "\n",
    "Build a convolutional neural network (a.k.a multilayer perceptron) with TensorFlow to classify images in the [Cifar-10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Neural Networks\n",
    "\n",
    "Convolutional cells pass kernels over the image, retaining spatial information between the pixels.\n",
    "![Convcell](src/images/conv_net.jpg \"Convolutional Cell\")\n",
    "\n",
    "Convolutional Neural Networks (CNNs) consist of one or more convolutional layers, typically followed by one or more dense (fully connected) layers. They often include pooling layers (downsampling) as seen below.\n",
    "\n",
    "![Convnet](src/images/CNN.png \"Typical Convolutional Neural Network Architecture\")\n",
    "\n",
    "\n",
    "### CIFAR-10 Dataset \n",
    "\n",
    "consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. \n",
    "There are 50000 training images and 10000 test images. \n",
    "The classes are airplane, automobile, bird, cat, deer, dog, frog, horse, ship, and truck.\n",
    "\n",
    "CIFAR-10 Example images\n",
    "![Cifar10](src/images/cifar10_ex.png \"Cifar-10 Example Images\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "import tarfile\n",
    "import pickle\n",
    "import numpy as np\n",
    "from six.moves import urllib\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Acquiring Data\n",
    "\n",
    "First, let's download the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Downloading cifar-10-python.tar.gz 100.0%\n",
      "Successfully downloaded cifar-10-python.tar.gz 170498071 bytes.\n"
     ]
    }
   ],
   "source": [
    "DATA_URL = 'http://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz'\n",
    "DATA_DIRECTORY = \"/tmp/cifar10-data\"\n",
    "\n",
    "def maybe_download_and_extract():\n",
    "    \"\"\"Download and extract the tarball from Alex Krizhevsky's website.\"\"\"\n",
    "    if not os.path.exists(DATA_DIRECTORY):\n",
    "        os.makedirs(DATA_DIRECTORY)\n",
    "    filename = DATA_URL.split('/')[-1]\n",
    "    filepath = os.path.join(DATA_DIRECTORY, filename)\n",
    "    if not os.path.exists(filepath):\n",
    "        def _progress(count, block_size, total_size):\n",
    "            sys.stdout.write('\\r>> Downloading %s %.1f%%' % (filename,\n",
    "                float(count * block_size) / float(total_size) * 100.0))\n",
    "            sys.stdout.flush()\n",
    "        filepath, _ = urllib.request.urlretrieve(DATA_URL, filepath, _progress)\n",
    "        print()\n",
    "        statinfo = os.stat(filepath)\n",
    "        print('Successfully downloaded', filename, statinfo.st_size, 'bytes.')\n",
    "    extracted_dir_path = os.path.join(DATA_DIRECTORY, 'cifar-10-batches-python')\n",
    "    if not os.path.exists(extracted_dir_path):\n",
    "        tarfile.open(filepath, 'r:gz').extractall(DATA_DIRECTORY)\n",
    "\n",
    "maybe_download_and_extract()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing data \n",
    "\n",
    "This data is stored in pickle files, so we will need to unpickle and open them. There are 5 batches of training data and one batch of testing data. Each of those files turns into a python dictionary when un-pickled, containing the training data, labels, a list of filenames corresponding to the data, and a batch label (for the training data). \n",
    "\n",
    "Both the images and labels need some preprocessing. \n",
    "The images come as a np.array with shape (10000, 3072) and can be reshaped to form a list of 3-channel (RGB) , 32x32 images with shape (10000, 3, 32, 32). Tensorflow wants images to be read in with channels last, so this array needs to have some of the dimensions transposed. Lastly, images are scaled so that pixel values range from 0 to 1 instead of 0 to 255. This is common practice \n",
    "\n",
    "The labels need to be one-hot encoded to be put into the model.\n",
    "\n",
    "Training data sets are concatenated into a single data set. If memory was an issue, data could be read in and transformed in batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def unpickle(file):\n",
    "    \"\"\"helper function to un-pickle data files\"\"\"\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict\n",
    "\n",
    "def reshape_data(ary):\n",
    "    \"\"\"helper function to reshape image arrays to tf dim ordering\"\"\"\n",
    "    ary = ary.reshape(-1, 3, 32, 32)\n",
    "    # put RGB channels last - this is what tensorflow wants, and matplotlib too!\n",
    "    return np.transpose(ary, axes = (0, 2, 3, 1))\n",
    "\n",
    "def one_hot_encode(ary, num_labels = 10):\n",
    "    \"\"\"helper function that encodes 1D label array\"\"\"\n",
    "    return np.eye(num_labels)[ary]\n",
    "    \n",
    "extracted_dir_path = os.path.join(DATA_DIRECTORY, 'cifar-10-batches-py')\n",
    "\n",
    "def extract_train_data():\n",
    "    images, labels = None, None\n",
    "    for i in range(1, 6):\n",
    "        filepath = extracted_dir_path+'/data_batch_{}'.format(i)\n",
    "        data_dict = unpickle(filepath)\n",
    "        temp_images = reshape_data(data_dict[b'data'])\n",
    "        temp_labels = one_hot_encode(data_dict[b'labels'])\n",
    "        if type(images) == np.ndarray:\n",
    "            images = np.concatenate([images, temp_images], axis = 0)\n",
    "            labels = np.concatenate([labels, temp_labels], axis = 0)            \n",
    "        else:\n",
    "            images, labels = temp_images, temp_labels\n",
    "    # change datatype so that tensorflow likes it, scale so pixel values range 0 to 1         \n",
    "    return (images/255).astype(np.float32), labels.astype(np.float32)\n",
    "\n",
    "def extract_test_data():\n",
    "    filepath = extracted_dir_path+'/test_batch'\n",
    "    data_dict = unpickle(filepath)\n",
    "    images = reshape_data(data_dict[b'data'])\n",
    "    labels = one_hot_encode(data_dict[b'labels'])\n",
    "    return (images/255).astype(np.float32), labels.astype(np.float32)\n",
    "\n",
    "images, labels = extract_train_data()\n",
    "\n",
    "test_images, test_labels = extract_test_data()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quick inspection\n",
    "\n",
    "Let's inspect a random image. Note that the image is very fuzzy...there are only 32 pixels of each side, which is about an order or magnitude less resolution than what you are used to!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f635173cf98>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f63521bc518>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n = 25348 # feel free to pick your own random number between 0 and 49999\n",
    "plt.imshow(images[n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image has label 8 or \"ship\"\n"
     ]
    }
   ],
   "source": [
    "label_names = ['airplane','automobile','bird','cat','deer','dog','frog','horse','ship','truck']\n",
    "label = labels[n].argmax()\n",
    "print('This image has label {} or \"{}\"'.format(label, label_names[label]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Histogram of image pixels, separated by color. The values range between 0 and 1 as expected. Note that these pixel distributions are very different from the MNIST dataset. These images are more complicated and will likely require a more complex model and more fitting time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.6335625 ,  0.93432708,  1.24217917,  1.33078958,  1.40803333,\n",
       "         1.24417917,  1.08178958,  0.79830208,  0.61402083,  0.71281667]),\n",
       " array([ 0. ,  0.1,  0.2,  0.3,  0.4,  0.5,  0.6,  0.7,  0.8,  0.9,  1. ]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f634f6ffeb8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(images[:,:,0].ravel(), color= 'r', alpha = 0.4, normed = True)\n",
    "plt.hist(images[:,:,1].ravel(), color= 'g', alpha = 0.4, normed = True)\n",
    "plt.hist(images[:,:,2].ravel(), color= 'b', alpha = 0.4, normed = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation Set\n",
    "\n",
    "Let's split off 10000 images from our training set to use as for validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation images shape (10000, 32, 32, 3)\n",
      "Validation labels shape (10000, 10)\n",
      "Train size 40000\n"
     ]
    }
   ],
   "source": [
    "VALIDATION_SIZE = 10000\n",
    "\n",
    "validation_images = images[:VALIDATION_SIZE, :, :, :]\n",
    "validation_labels = labels[:VALIDATION_SIZE, :]\n",
    "train_images = images[VALIDATION_SIZE:, :, :, :]\n",
    "train_labels = labels[VALIDATION_SIZE:, :]\n",
    "\n",
    "train_size = train_labels.shape[0]\n",
    "\n",
    "print('Validation images shape', validation_images.shape)\n",
    "print('Validation labels shape', validation_labels.shape)\n",
    "print('Train size', train_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Subset the Data\n",
    "\n",
    "CIFAR-10 is a large dataset and takes a long time to train. So that you have time to iterate on this model, we will downsample the dataset. When you have a model you like, you should try it with all of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get indicies of data that we will keep in our subsample\n",
    "train_subsample_inds = np.random.choice(train_size, train_size//10, replace = False)\n",
    "validation_subsample_inds = np.random.choice(VALIDATION_SIZE, VALIDATION_SIZE//10, replace = False)\n",
    "test_subsample_inds = np.random.choice(VALIDATION_SIZE, VALIDATION_SIZE//10, replace = False)\n",
    "\n",
    "# create those subsamples \n",
    "train_images_small = train_images[train_subsample_inds]\n",
    "train_labels_small = train_labels[train_subsample_inds]\n",
    "\n",
    "validation_images_small = validation_images[validation_subsample_inds]\n",
    "validation_labels_small = validation_labels[validation_subsample_inds]\n",
    "\n",
    "test_images_small = test_images[test_subsample_inds]\n",
    "test_labels_small = test_labels[test_subsample_inds]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling\n",
    "\n",
    "These next few cells set up things for the model\n",
    "\n",
    "First, we'll set a few parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# these shouldn't change\n",
    "IMAGE_SIZE = 32\n",
    "NUM_CHANNELS = 3\n",
    "NUM_LABELS = 10\n",
    "\n",
    "# these could be tuned \n",
    "BATCH_SIZE = 50\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, create placeholder tensors that will be used to feed data into the model. Remember that the first dimension should allow for flexibility for different batch sizes (and evaluating the validation and test data, as well)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create placeholder tensors for X and Y. Remember to specify a data type\n",
    "X = tf.placeholder(tf.float32, shape=(None, IMAGE_SIZE, IMAGE_SIZE, NUM_CHANNELS))\n",
    "Y = tf.placeholder(tf.float32, shape=(None, NUM_LABELS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up your model weights\n",
    "\n",
    "Now, fill in variable tensors for the model weights. \n",
    "\n",
    "Your convolutional weights should have the shape [x_kernel_size, y_kernel_size, input_channels, output_channels]\n",
    "\n",
    "The first fully connected layer weights are given to you. The shape assumes that a max pool with size [2, 2] has happened after each convolutional layer. This reduces each image by a factor of 4 in the X and Y dimensions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conv_weights = {'c1' : tf.Variable(tf.truncated_normal([5, 5, NUM_CHANNELS, 64], stddev=0.05, seed=SEED)),  # 5x5 filter, depth 64\n",
    "                'c2' : tf.Variable(tf.truncated_normal([5, 5, 64, 64], stddev=0.05, seed=SEED))\n",
    "               }\n",
    "\n",
    "conv_biases = {'cb1' : tf.Variable(tf.constant(0.1, shape=[64])), # start with a tensor with all values = 0.1\n",
    "               'cb2' : tf.Variable(tf.constant(0.1, shape=[64]))\n",
    "              }\n",
    "\n",
    "fc_weights = {'fc1' : tf.Variable(tf.truncated_normal([IMAGE_SIZE // 4 * IMAGE_SIZE // 4 * 64, 384], stddev=0.05, seed=SEED)),\n",
    "              'fc2' : tf.Variable(tf.truncated_normal([384, 192], stddev=0.05,seed=SEED)),\n",
    "              'fc3' : tf.Variable(tf.truncated_normal([192, NUM_LABELS], stddev=0.05, seed = SEED)),\n",
    "             }\n",
    "\n",
    "fc_biases = {'fcb1' : tf.Variable(tf.constant(0.1, shape=[384])),\n",
    "            'fcb2' : tf.Variable(tf.constant(0.1, shape=[192])),\n",
    "            'fcb3' : tf.Variable(tf.constant(0.1, shape=[NUM_LABELS]))\n",
    "            }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Put together your model graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def feed_forward(data, train=False):\n",
    "    \"\"\"The Model definition.\"\"\"\n",
    "    # 2D convolution, with 'SAME' padding (i.e. the output feature map has\n",
    "    # the same size as the input). Note that {strides} is a 4D array whose\n",
    "    # shape matches the data layout: [image index, y, x, depth].\n",
    "    conv1 = tf.nn.conv2d(data,\n",
    "                        conv_weights['c1'],\n",
    "                        strides=[1, 1, 1, 1],\n",
    "                        padding='SAME')\n",
    "\n",
    "    # Bias and rectified linear non-linearity.\n",
    "    relu1 = tf.nn.relu(tf.nn.bias_add(conv1, conv_biases['cb1']))\n",
    "\n",
    "    # Max pooling. The kernel size spec ksize also follows the layout of\n",
    "    # the data. Here we have a pooling window of 2, and a stride of 2.\n",
    "    pool1 = tf.nn.max_pool(relu1,\n",
    "                          ksize=[1, 3, 3, 1],\n",
    "                          strides=[1, 2, 2, 1],\n",
    "                          padding='SAME')\n",
    "    \n",
    "    norm1 = tf.nn.lrn(pool1, 4, bias=1.0, alpha=0.001 / 9.0, beta=0.75, name='norm1')\n",
    "    \n",
    "    # second convolutional layer, activation, pooling, and normalization\n",
    "    conv2 = tf.nn.conv2d(norm1,\n",
    "                        conv_weights['c2'],\n",
    "                        strides=[1, 1, 1, 1],\n",
    "                        padding='SAME')\n",
    "    relu2 = tf.nn.relu(tf.nn.bias_add(conv2, conv_biases['cb1']))\n",
    "    norm2 = tf.nn.lrn(relu2, 4, bias=1.0, alpha=0.001 / 9.0, beta=0.75, name='norm2')\n",
    "    \n",
    "    pool2 = tf.nn.max_pool(norm2, ksize=[1, 3, 3, 1],\n",
    "                          strides=[1, 2, 2, 1], \n",
    "                          padding='SAME', name='pool2')\n",
    "\n",
    "\n",
    "    # Reshape the feature map cuboid into a 2D matrix to feed it to the\n",
    "    # fully connected layers.\n",
    "    pool_shape = pool2.get_shape().as_list()\n",
    "    reshape = tf.reshape(pool2, [tf.shape(pool2)[0], int(pool_shape[1] * pool_shape[2] * pool_shape[3])])\n",
    "\n",
    "  \n",
    "    # Fully connected layer. Note that the '+' operation automatically\n",
    "    # broadcasts the biases.\n",
    "    fc1 = tf.nn.relu(tf.matmul(reshape, fc_weights['fc1']) + fc_biases['fcb1'])\n",
    "    \n",
    "    fc2 = tf.nn.relu(tf.matmul(fc1, fc_weights['fc2']) + fc_biases['fcb2'])\n",
    "\n",
    "    # Add a 50% dropout during training only. Dropout also scales\n",
    "    # activations such that no rescaling is needed at evaluation time.\n",
    "    if train:\n",
    "        fc1 = tf.nn.dropout(fc1, 0.5, seed=SEED)\n",
    "        fc2 = tf.nn.dropout(fc2, 0.5, seed=SEED)\n",
    "        \n",
    "    return tf.matmul(fc2, fc_weights['fc3']) + fc_biases['fcb3']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model prediction\n",
    "\n",
    "Use the feed_forward operation to get predictions on your inputs (X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logits = feed_forward(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss operation\n",
    "\n",
    "Define your loss operation. This is a multi-class classification, just like the previous exercise....you should refer back if you aren't sure what to do here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define the loss operation\n",
    "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimizer and training operation\n",
    "\n",
    "Define an optimizer and a train_op. I suggest starting with the [Adam Optimizer](https://www.tensorflow.org/api_docs/python/tf/train/AdamOptimizer) with the parameters below, but you can try different optimizers as you tune this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Optimizer hyperparameters, can be tuned\n",
    "LEARNING_RATE = 0.002\n",
    "BETA1 = 0.9\n",
    "BETA2 = 0.999\n",
    "EPSILON = 1e-08\n",
    "\n",
    "# Define the optimizer operation\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=LEARNING_RATE,\n",
    "                                  beta1=BETA1,\n",
    "                                  beta2=BETA2,\n",
    "                                  epsilon=EPSILON,\n",
    "                                  )\n",
    "\n",
    "train_op = optimizer.minimize(loss_op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy metric\n",
    "\n",
    "Since the loss function is a little hard to interpret, let's code an accuracy metric so we can get a better idea how our model is doing. We also did this in the previous exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define a custom accuracy operation\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the variables!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize the variables (i.e. assign their default value)\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model \n",
    "\n",
    "Fill out the function below to train your model using the subsampled data. (Use the subsets in the feed dicts!) \n",
    "\n",
    "Note that with the model parameters as defined, this model will overfit to the training set, even with regularization. Once you get your code running, you should change the model hyperparamters and/or architecture to try and improve your results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_subset(num_epochs): \n",
    "    \"\"\"Function that trains model on subsets of CIFAR10 data\"\"\"\n",
    "    train_size = train_labels_small.shape[0]\n",
    "    steps = num_epochs * train_size // BATCH_SIZE\n",
    "    steps_per_epoch = train_size // BATCH_SIZE\n",
    "          \n",
    "    with tf.Session() as sess:\n",
    "\n",
    "        # Run the initializer\n",
    "        sess.run(init)    \n",
    "          \n",
    "        for step in range(steps):\n",
    "            # Compute the offset of the current minibatch in the data.\n",
    "            # Note that we could use better randomization across epochs.\n",
    "            offset = (step * BATCH_SIZE) % (train_size - BATCH_SIZE)\n",
    "            batch_data = train_images_small[offset:(offset + BATCH_SIZE), :, :, :]\n",
    "            batch_labels = train_labels_small[offset:(offset + BATCH_SIZE)]\n",
    "          \n",
    "            # Run the training operation to update the weights, use a feed_dict to use the batches you created above\n",
    "            sess.run(train_op, feed_dict={X: batch_data, Y: batch_labels})\n",
    "        \n",
    "            # display output if desired\n",
    "            if step % steps_per_epoch == 0:\n",
    "                # Calculate batch loss and accuracy\n",
    "                # You can run multiple operations using a list\n",
    "                loss, acc = sess.run([loss_op, accuracy], feed_dict={X: batch_data,\n",
    "                                                                     Y: batch_labels})\n",
    "                print(\"Epoch {}, Minibatch Loss = {:.4f}, Training Accuracy = {:.3f}\".format(str(step//steps_per_epoch),\n",
    "                                                                               loss, \n",
    "                                                                               acc)) \n",
    "            \n",
    "                val_acc = sess.run(accuracy, feed_dict = {X: validation_images_small,\n",
    "                                                          Y: validation_labels_small})\n",
    "                print(\"Validation accuracy : {}\".format(val_acc))\n",
    "\n",
    "        print(\"Optimization Finished!\")\n",
    "          \n",
    "        # Run the accuracy operations for the CIFAR10 test images\n",
    "        test_acc = sess.run(accuracy, feed_dict={X: test_images_small,\n",
    "                                                 Y: test_labels_small})\n",
    "        print(\"Test Accuracy : {}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Minibatch Loss = 2.5434, Training Accuracy = 0.140\n",
      "Validation accuracy : 0.0949999988079071\n",
      "Epoch 1, Minibatch Loss = 1.9938, Training Accuracy = 0.300\n",
      "Validation accuracy : 0.257999986410141\n",
      "Epoch 2, Minibatch Loss = 1.3816, Training Accuracy = 0.420\n",
      "Validation accuracy : 0.4050000011920929\n",
      "Epoch 3, Minibatch Loss = 1.5658, Training Accuracy = 0.480\n",
      "Validation accuracy : 0.4189999997615814\n",
      "Epoch 4, Minibatch Loss = 1.4196, Training Accuracy = 0.480\n",
      "Validation accuracy : 0.39800000190734863\n",
      "Epoch 5, Minibatch Loss = 1.2586, Training Accuracy = 0.560\n",
      "Validation accuracy : 0.43799999356269836\n",
      "Epoch 6, Minibatch Loss = 1.3181, Training Accuracy = 0.540\n",
      "Validation accuracy : 0.3779999911785126\n",
      "Epoch 7, Minibatch Loss = 0.8776, Training Accuracy = 0.700\n",
      "Validation accuracy : 0.43700000643730164\n",
      "Epoch 8, Minibatch Loss = 1.1379, Training Accuracy = 0.640\n",
      "Validation accuracy : 0.4480000138282776\n",
      "Epoch 9, Minibatch Loss = 0.8644, Training Accuracy = 0.700\n",
      "Validation accuracy : 0.453000009059906\n",
      "Epoch 10, Minibatch Loss = 0.7800, Training Accuracy = 0.700\n",
      "Validation accuracy : 0.46700000762939453\n",
      "Epoch 11, Minibatch Loss = 0.6397, Training Accuracy = 0.780\n",
      "Validation accuracy : 0.4449999928474426\n",
      "Epoch 12, Minibatch Loss = 0.6063, Training Accuracy = 0.780\n",
      "Validation accuracy : 0.4259999990463257\n",
      "Epoch 13, Minibatch Loss = 0.6050, Training Accuracy = 0.800\n",
      "Validation accuracy : 0.46799999475479126\n",
      "Epoch 14, Minibatch Loss = 0.4781, Training Accuracy = 0.900\n",
      "Validation accuracy : 0.42899999022483826\n",
      "Epoch 15, Minibatch Loss = 0.6607, Training Accuracy = 0.760\n",
      "Validation accuracy : 0.4320000112056732\n",
      "Epoch 16, Minibatch Loss = 0.5321, Training Accuracy = 0.780\n",
      "Validation accuracy : 0.4269999861717224\n",
      "Epoch 17, Minibatch Loss = 0.2490, Training Accuracy = 0.920\n",
      "Validation accuracy : 0.4339999854564667\n",
      "Epoch 18, Minibatch Loss = 0.4360, Training Accuracy = 0.840\n",
      "Validation accuracy : 0.44200000166893005\n",
      "Epoch 19, Minibatch Loss = 0.2430, Training Accuracy = 0.940\n",
      "Validation accuracy : 0.42500001192092896\n",
      "Optimization Finished!\n",
      "Test Accuracy : 0.43799999356269836\n"
     ]
    }
   ],
   "source": [
    "train_subset(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train on the full dataset\n",
    "\n",
    "When you have an architecture you like, try training the model on the full dataset. This function should look just like the train_subset function, but you should use the full training, validation, and test sets in the feed dict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(num_epochs): \n",
    "    \"\"\"Function that trains model on subsets of CIFAR10 data\"\"\"\n",
    "    train_size = train_labels_small.shape[0]\n",
    "    steps = num_epochs * train_size // BATCH_SIZE\n",
    "    steps_per_epoch = train_size // BATCH_SIZE\n",
    "          \n",
    "    with tf.Session() as sess:\n",
    "\n",
    "        # Run the initializer\n",
    "        sess.run(init)    \n",
    "          \n",
    "        for step in range(steps):\n",
    "            # Compute the offset of the current minibatch in the data.\n",
    "            # Note that we could use better randomization across epochs.\n",
    "            offset = (step * BATCH_SIZE) % (train_size - BATCH_SIZE)\n",
    "            batch_data = train_images[offset:(offset + BATCH_SIZE), :, :, :]\n",
    "            batch_labels = train_labels[offset:(offset + BATCH_SIZE)]\n",
    "          \n",
    "            # Run the training operation to update the weights, use a feed_dict to use the batches you created above\n",
    "            sess.run(train_op, feed_dict={X: batch_data, Y: batch_labels})\n",
    "        \n",
    "            # display output if desired\n",
    "            if step % steps_per_epoch == 0:\n",
    "                # Calculate batch loss and accuracy\n",
    "                # You can run multiple operations using a list\n",
    "                loss, acc = sess.run([loss_op, accuracy], feed_dict={X: batch_data,\n",
    "                                                                     Y: batch_labels})\n",
    "                print(\"Epoch \" + str(step//steps_per_epoch) + \", Minibatch Loss= \" + \\\n",
    "                      \"{:.4f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "                      \"{:.3f}\".format(acc))\n",
    "            \n",
    "                val_acc = sess.run(accuracy, feed_dict = {X: validation_images,\n",
    "                                                            Y: validation_labels})\n",
    "                print(\"Validation accuracy : {}\".format(val_acc))\n",
    "\n",
    "        print(\"Optimization Finished!\")\n",
    "          \n",
    "        # Run the accuracy operations for the CIFAR10 test images\n",
    "        test_acc = sess.run(accuracy, feed_dict={X: test_images,\n",
    "                                                 Y: test_labels})\n",
    "        print(\"Test Accuracy : {}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Minibatch Loss= 2.9238, Training Accuracy= 0.220\n",
      "Validation accuracy : 0.10300000011920929\n",
      "Epoch 1, Minibatch Loss= 1.8498, Training Accuracy= 0.360\n",
      "Validation accuracy : 0.299699991941452\n",
      "Epoch 2, Minibatch Loss= 1.5656, Training Accuracy= 0.520\n",
      "Validation accuracy : 0.3528999984264374\n",
      "Epoch 3, Minibatch Loss= 1.9043, Training Accuracy= 0.320\n",
      "Validation accuracy : 0.39719998836517334\n",
      "Epoch 4, Minibatch Loss= 1.7436, Training Accuracy= 0.340\n",
      "Validation accuracy : 0.4115999937057495\n",
      "Epoch 5, Minibatch Loss= 1.4417, Training Accuracy= 0.500\n",
      "Validation accuracy : 0.4174000024795532\n",
      "Epoch 6, Minibatch Loss= 1.1176, Training Accuracy= 0.660\n",
      "Validation accuracy : 0.4203000068664551\n",
      "Epoch 7, Minibatch Loss= 1.4612, Training Accuracy= 0.480\n",
      "Validation accuracy : 0.4309999942779541\n",
      "Epoch 8, Minibatch Loss= 1.2230, Training Accuracy= 0.580\n",
      "Validation accuracy : 0.4189000129699707\n",
      "Epoch 9, Minibatch Loss= 1.1756, Training Accuracy= 0.520\n",
      "Validation accuracy : 0.40950000286102295\n",
      "Epoch 10, Minibatch Loss= 1.0004, Training Accuracy= 0.620\n",
      "Validation accuracy : 0.41119998693466187\n",
      "Epoch 11, Minibatch Loss= 1.2063, Training Accuracy= 0.600\n",
      "Validation accuracy : 0.43560001254081726\n",
      "Epoch 12, Minibatch Loss= 0.8151, Training Accuracy= 0.700\n",
      "Validation accuracy : 0.4433000087738037\n",
      "Epoch 13, Minibatch Loss= 0.9959, Training Accuracy= 0.600\n",
      "Validation accuracy : 0.43160000443458557\n",
      "Epoch 14, Minibatch Loss= 0.7384, Training Accuracy= 0.800\n",
      "Validation accuracy : 0.4399999976158142\n",
      "Epoch 15, Minibatch Loss= 0.4968, Training Accuracy= 0.800\n",
      "Validation accuracy : 0.4284999966621399\n",
      "Epoch 16, Minibatch Loss= 0.4339, Training Accuracy= 0.900\n",
      "Validation accuracy : 0.412200003862381\n",
      "Epoch 17, Minibatch Loss= 0.5674, Training Accuracy= 0.820\n",
      "Validation accuracy : 0.4056999981403351\n",
      "Epoch 18, Minibatch Loss= 0.3572, Training Accuracy= 0.880\n",
      "Validation accuracy : 0.41429999470710754\n",
      "Epoch 19, Minibatch Loss= 0.2828, Training Accuracy= 0.880\n",
      "Validation accuracy : 0.4108000099658966\n",
      "Optimization Finished!\n",
      "Test Accuracy : 0.39800000190734863\n"
     ]
    }
   ],
   "source": [
    "train(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
