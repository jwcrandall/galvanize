{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple seq2seq example using LSTMs with TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on Matvey Ezhov's [materials](https://github.com/ematvey/tensorflow-seq2seq-tutorials) under the [MIT License](https://opensource.org/licenses/MIT)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This exercise helps you implement a basic seq2seq model comprised of a forward-only encoder - decoder, based on the architecture described in [Sutskever, Vinyals and Le (2014)](https://arxiv.org/abs/1409.3215)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Architecture diagram from their paper:\n",
    "![seq2seq architecutre](pictures/1-seq2seq.png)\n",
    "Rectangles are encoder and decoder's recurrent layers. The sequence `[A, B, C]` is the **encoder input**.  The encoder's hidden state changes while reading the sequence.  After input sequence ends (indicated by `<EOS>`), the encoder passes its final state to decoder, which receives `[<EOS>, W, X, Y, Z]` as the **decoder input** and is trained to the **decoder output** `[W, X, Y, Z, <EOS>]`. `<EOS>` token is a special word in vocabulary that signals to decoder the beginning of translation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vocabulary\n",
    "\n",
    "Seq2seq maps one sequence onto another sequence. Both sequences consist of integers from a fixed range. In language tasks, integers usually correspond to words: we first construct a vocabulary by assigning to every word in our corpus a serial integer. First few integers are reserved for special tokens. We'll call the upper bound on vocabulary a `vocabulary size`.\n",
    "\n",
    "A lot of processing goes into making text into numbers.  To simplify things, this notebook will focus instead on trying to learn a sequence of numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# For instance, here are four separate sequences, all of different lengths\n",
    "x = [[5, 7, 8], [6, 3], [3], [1]]\n",
    "for i, seq in enumerate(x):\n",
    "    print(\"Sequence {0}: {1}\".format(i,seq))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While manipulating such variable-length lists are convenient to humans, RNNs prefer a different layout called **time-major** where sequences are found in the columns of a matrix of size `[max_time, batch_size].`  Sequences shorter than the longest one are padded with zeros at the end. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import helpers  # uses helper file - check out at you leisure\n",
    "xt, xlen = helpers.batch(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time 0: [5 6 3 1]\n",
      "Time 1: [7 3 0 0]\n",
      "Time 2: [8 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "for i, batch in enumerate(xt):\n",
    "    print(\"Time {0}: {1}\".format(i,batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of each sequence in each column:\n",
      "[3, 2, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "print(\"The length of each sequence in each column:\")\n",
    "print(xlen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple seq2seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The encoder (an LSTM) starts with an initial state and runs through the input sequence until it reaches its `final_state`.\n",
    "\n",
    "The decoder uses the encoder's `final_state` as its `initial_state`.\n",
    "\n",
    "Therefore there are **encoder inputs** and **decoder inputs** and **decoder targets.** The decoder's outputs are mapped onto the output space using a `[hidden_units x output_vocab_size]` size matrix. \n",
    "\n",
    "Both the encoder and decoder are trying to learn sequences.  Tensorflow's [LSTMCells](https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/LSTMCell) will be used to model the sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "# import numpy, tensorflow, and also import helpers (like we did above in this worksheet)\n",
    "# remember conventions for importing numpy and tensorflow\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.4.1'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# what version of tensorflow are you running?\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model inputs and outputs "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First critical thing to decide: vocabulary size.\n",
    "\n",
    "Dynamic RNN models can be adapted to different batch sizes and sequence lengths without retraining, but changing vocabulary size requires retraining the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PAD = 0 # if a sequence is short, what should it be padded with to get it up to some desired length?\n",
    "EOS = 1 # what signifies the end of a sequence\n",
    "\n",
    "vocab_size = 10  # how many different single digit numbers could we see? 0,1,2,3,5,6,7,8,9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the tensors\n",
    "Remember to use tf.placeholder.  Generally the shape will be `(max_time, batch_size)` but the values of max_time and batch size may change so we want to initialize them with [unknown shapes.](https://www.tensorflow.org/api_docs/python/tf/TensorShape) For datatype use 32 bit integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoder_inputs = tf.placeholder(shape=(None, None), dtype=tf.int32, name='encoder_inputs')\n",
    "decoder_inputs = tf.placeholder(shape=(None, None), dtype=tf.int32, name='decoder_inputs')\n",
    "decoder_targets = tf.placeholder(shape=(None, None), dtype=tf.int32, name='decoder_targets')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddings\n",
    "\n",
    "Let's give the `encoder` and `decoder` more meaningful inputs than just the sequence numbers. We can embed context (i.e. if 4 then 5 happens then 1 is probably next) into the inputs.  This process of embedding will take vectors that are sparse (filled with many zeros) and make then dense, which is required for the encoder and decoder RNNs.  Specifics of working with embeddings are nicely described in [official tutorial on embeddings](https://www.tensorflow.org/tutorials/word2vec/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First initialize the embedding matrix. Initializations are random. We rely on our end-to-end training to learn vector representations for the embeddings for the sequences of numbers jointly with encoder and decoder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize the embeddings as a tensorflow variable using a [random uniform](https://www.tensorflow.org/api_docs/python/tf/random_uniform) distribution between -1.0 and 1.0, datatype float.  Its shape should be (vocab_size, input_embedding_size)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_embedding_size = 8 # how many columns we think we'll need to capture the relationship betweeen the numbers\n",
    "embeddings = tf.Variable(tf.random_uniform([vocab_size, input_embedding_size], -1.0, 1.0), dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the operations\n",
    "Start with the embeddings.  In this case the seq2seq network will just be learning a sequence of digits, so the encoder inputs and the decoders inputs (and embeddings) are the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoder_inputs_embedded = tf.nn.embedding_lookup(embeddings, encoder_inputs)\n",
    "decoder_inputs_embedded = tf.nn.embedding_lookup(embeddings, decoder_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder\n",
    "\n",
    "The encoder is a [dynamic rnn](https://www.tensorflow.org/versions/master/api_docs/python/tf/nn/dynamic_rnn) comprised of [LSTMCells.](https://www.tensorflow.org/versions/master/api_docs/python/tf/contrib/rnn/LSTMCell) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoder_hidden_units = 20 # how many units in the LSTM cell?\n",
    "\n",
    "encoder_cell = tf.contrib.rnn.LSTMCell(encoder_hidden_units)\n",
    "\n",
    "_, encoder_final_state = tf.nn.dynamic_rnn(encoder_cell, \n",
    "                                           encoder_inputs_embedded,\n",
    "                                           dtype=tf.float32,\n",
    "                                           time_major=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `encoder_final_state` is also called \"thought vector\".  In this simple seq2seq model this is the only point where Encoder passes information to Decoder.  Backpropagation through time (BPTT) algorithm will tune the model to pass enough information throught the thought vector for correct sequence output decoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTMStateTuple(c=<tf.Tensor 'rnn/while/Exit_2:0' shape=(?, 20) dtype=float32>, h=<tf.Tensor 'rnn/while/Exit_3:0' shape=(?, 20) dtype=float32>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_final_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder\n",
    "\n",
    "The decoder is also a [dynamic rnn](https://www.tensorflow.org/versions/master/api_docs/python/tf/nn/dynamic_rnn) comprised of [LSTMCells.](https://www.tensorflow.org/versions/master/api_docs/python/tf/contrib/rnn/LSTMCell) \n",
    "Give it the same number of hidden units as the encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "decoder_hidden_units = encoder_hidden_units\n",
    "\n",
    "decoder_cell = tf.contrib.rnn.LSTMCell(decoder_hidden_units)\n",
    "\n",
    "decoder_outputs, decoder_final_state = tf.nn.dynamic_rnn(decoder_cell,\n",
    "                                                         decoder_inputs_embedded,\n",
    "                                                         initial_state=encoder_final_state,\n",
    "                                                         dtype=tf.float32, \n",
    "                                                         time_major=True,\n",
    "                                                         scope=\"plain_decoder\",)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point `decoder_cell` output is a `hidden_units` sized vector at every timestep. However, for training and prediction we need logits of size `vocab_size`. Reasonable thing would be to put linear layer (fully-connected layer without activation function) on top of LSTM output to get non-normalized logits. This layer is called projection layer by convention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "decoder_logits = tf.contrib.layers.linear(decoder_outputs, vocab_size)\n",
    "decoder_prediction = tf.argmax(decoder_logits, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'fully_connected/BiasAdd:0' shape=(?, ?, 10) dtype=float32>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNN outputs tensor of shape `[max_time, batch_size, hidden_units]` which projection layer maps onto `[max_time, batch_size, vocab_size]`. `vocab_size` part of the shape is static, while `max_time` and `batch_size` are dynamic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Loss and Optimizer\n",
    "For the loss get the [mean](https://www.tensorflow.org/versions/master/api_docs/python/tf/reduce_mean) stepwise_cross_entropy.  For the optimizer, think about using [Adam.](https://www.tensorflow.org/versions/master/api_docs/python/tf/train/AdamOptimizer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stepwise_cross_entropy = tf.nn.softmax_cross_entropy_with_logits(\n",
    "    labels=tf.one_hot(decoder_targets, depth=vocab_size, dtype=tf.float32),\n",
    "    logits=decoder_logits,\n",
    ")\n",
    "\n",
    "loss = tf.reduce_mean(stepwise_cross_entropy)\n",
    "train_op = tf.train.AdamOptimizer().minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make an initialize operation to initialize the variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize the variables (i.e. assign their default value)\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training on a toy data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will teach our model to memorize and reproduce input sequence. Sequences will be random, with varying length.\n",
    "\n",
    "Since random sequences do not contain any structure, model will not be able to exploit any patterns in data. It will simply encode sequence in a thought vector, then decode from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one batch:\n",
      "[3, 2, 6, 5, 8, 5, 9]\n",
      "[6, 8, 2]\n",
      "[7, 2, 6, 9, 8]\n",
      "[2, 4, 8]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 4\n",
    "\n",
    "batches = helpers.random_sequences(length_from=3, length_to=8,\n",
    "                                   vocab_lower=2, vocab_upper=10,\n",
    "                                   batch_size=batch_size)\n",
    "print('one batch:')\n",
    "for seq in next(batches):\n",
    "    print(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from the batch it determines the encoder_input, the decoder_input, and the decoder_target\n",
    "def next_feed(prnt=False):\n",
    "    batch = next(batches)\n",
    "    if prnt:\n",
    "        print(\"The batch is:\")\n",
    "        for line in batch:\n",
    "            print(line)\n",
    "    encoder_inputs_, _ = helpers.batch(batch)\n",
    "    decoder_targets_, _ = helpers.batch(\n",
    "        [(sequence) + [EOS] for sequence in batch]\n",
    "    )\n",
    "    decoder_inputs_, _ = helpers.batch(\n",
    "        [[EOS] + (sequence) for sequence in batch]\n",
    "    )\n",
    "    return {\n",
    "        encoder_inputs: encoder_inputs_,\n",
    "        decoder_inputs: decoder_inputs_,\n",
    "        decoder_targets: decoder_targets_,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The batch is:\n",
      "[5, 9, 4, 3, 4, 2]\n",
      "[3, 9, 5, 7, 2, 3]\n",
      "[8, 2, 3, 3]\n",
      "[2, 8, 7, 9, 5]\n"
     ]
    }
   ],
   "source": [
    "test=next_feed(prnt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5 3 8 2]\n",
      " [9 9 2 8]\n",
      " [4 5 3 7]\n",
      " [3 7 3 9]\n",
      " [4 2 0 5]\n",
      " [2 3 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(test[encoder_inputs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 1 1]\n",
      " [5 3 8 2]\n",
      " [9 9 2 8]\n",
      " [4 5 3 7]\n",
      " [3 7 3 9]\n",
      " [4 2 0 5]\n",
      " [2 3 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(test[decoder_inputs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5 3 8 2]\n",
      " [9 9 2 8]\n",
      " [4 5 3 7]\n",
      " [3 7 3 9]\n",
      " [4 2 1 5]\n",
      " [2 3 0 1]\n",
      " [1 1 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(test[decoder_targets])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start the session and train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0\n",
      "****************************************\n",
      "  minibatch loss: 2.286\n",
      "  sample 1:\n",
      "    input     > [6 6 2 4 6 0 0 0]\n",
      "    predicted > [6 6 3 5 5 3 3 4 4]\n",
      "  sample 2:\n",
      "    input     > [6 5 4 8 5 7 4 9]\n",
      "    predicted > [2 3 2 3 3 3 4 3 3]\n",
      "  sample 3:\n",
      "    input     > [8 9 8 5 0 0 0 0]\n",
      "    predicted > [6 6 6 3 3 4 4 4 4]\n",
      "\n",
      "batch 500\n",
      "****************************************\n",
      "  minibatch loss: 0.882\n",
      "  sample 1:\n",
      "    input     > [7 3 3 3 3 3 0 0]\n",
      "    predicted > [3 3 3 3 3 5 1 0 0]\n",
      "  sample 2:\n",
      "    input     > [2 6 2 2 6 0 0 0]\n",
      "    predicted > [2 2 2 2 1 1 0 0 0]\n",
      "  sample 3:\n",
      "    input     > [9 4 9 3 6 2 0 0]\n",
      "    predicted > [6 6 6 6 6 2 1 0 0]\n",
      "\n",
      "batch 1000\n",
      "****************************************\n",
      "  minibatch loss: 0.585\n",
      "  sample 1:\n",
      "    input     > [2 8 5 7 6 0 0 0]\n",
      "    predicted > [2 8 5 7 6 1 0 0 0]\n",
      "  sample 2:\n",
      "    input     > [2 7 9 6 5 9 8 5]\n",
      "    predicted > [2 9 9 6 8 9 8 1 1]\n",
      "  sample 3:\n",
      "    input     > [7 8 6 9 3 9 6 0]\n",
      "    predicted > [7 6 6 9 9 9 9 1 0]\n",
      "\n",
      "batch 1500\n",
      "****************************************\n",
      "  minibatch loss: 0.369\n",
      "  sample 1:\n",
      "    input     > [5 7 9 0 0 0 0 0]\n",
      "    predicted > [5 7 9 1 0 0 0 0 0]\n",
      "  sample 2:\n",
      "    input     > [5 8 5 5 4 0 0 0]\n",
      "    predicted > [5 5 5 5 4 1 0 0 0]\n",
      "  sample 3:\n",
      "    input     > [7 3 8 2 6 4 6 0]\n",
      "    predicted > [7 4 8 6 6 6 6 1 0]\n",
      "\n",
      "batch 2000\n",
      "****************************************\n",
      "  minibatch loss: 0.343\n",
      "  sample 1:\n",
      "    input     > [7 3 3 5 2 2 4 5]\n",
      "    predicted > [7 3 5 5 2 2 4 5 1]\n",
      "  sample 2:\n",
      "    input     > [4 9 7 3 8 3 8 0]\n",
      "    predicted > [3 9 7 3 8 3 8 1 0]\n",
      "  sample 3:\n",
      "    input     > [4 8 4 9 9 3 6 7]\n",
      "    predicted > [4 8 4 9 9 3 6 7 1]\n",
      "\n",
      "batch 2500\n",
      "****************************************\n",
      "  minibatch loss: 0.222\n",
      "  sample 1:\n",
      "    input     > [2 6 8 9 5 8 2 0]\n",
      "    predicted > [2 8 8 9 5 6 2 1 0]\n",
      "  sample 2:\n",
      "    input     > [5 8 8 7 0 0 0 0]\n",
      "    predicted > [5 8 8 7 1 0 0 0 0]\n",
      "  sample 3:\n",
      "    input     > [8 3 5 6 8 5 4 6]\n",
      "    predicted > [8 3 5 6 8 5 4 6 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "batch_size = 50\n",
    "\n",
    "batches = helpers.random_sequences(length_from=3, length_to=8,\n",
    "                                   vocab_lower=2, vocab_upper=10,\n",
    "                                   batch_size=batch_size)\n",
    "\n",
    "loss_track = []\n",
    "\n",
    "# Start training\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    # Run the initializer\n",
    "    sess.run(init)\n",
    "\n",
    "    max_batches = 2501\n",
    "    print_interval = 500\n",
    "\n",
    "    for batch in range(max_batches):\n",
    "        fd = next_feed()\n",
    "        _, l = sess.run([train_op, loss], fd)\n",
    "        loss_track.append(l)\n",
    "\n",
    "        if batch == 0 or batch % print_interval == 0:\n",
    "            print('batch {}'.format(batch))\n",
    "            print('*'*40)\n",
    "            print('  minibatch loss: {0:0.3f}'.format(sess.run(loss, fd)))\n",
    "            predict_ = sess.run(decoder_prediction, fd)\n",
    "            for i, (inp, pred) in enumerate(zip(fd[encoder_inputs].T, predict_.T)):\n",
    "                print('  sample {}:'.format(i + 1))\n",
    "                print('    input     > {}'.format(inp))\n",
    "                print('    predicted > {}'.format(pred))\n",
    "                if i >= 2:\n",
    "                    break\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.2240 after 125050 examples (batch_size=50)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe937884710>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(loss_track)\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "print('loss {:.4f} after {} examples (batch_size={})'.format(loss_track[-1], len(loss_track)*batch_size, batch_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Something is definitely getting learned!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next steps:\n",
    "Try changing the number of hidden units or the embedding size and see what happens to your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
