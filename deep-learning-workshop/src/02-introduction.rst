.. introduction 


Introduction
============

A brief history of Neural Networks
----------------------------------
The idea of Artificial Neural Networks (ANNs) was introduced by Warren McCulloch and Walter Pitts in 1943 in their paper `A Logical Calculus of Ideas Immanent in Nervous Activity. <https://goo.gl/Ul4mxW>`_  This paper introduced the concept of a neuron as a computational unit whose output was an all-or-nothing response to input from upstream neurons. With the analogy that the quality of the connection between two neurons could be quantified as weights, they showed a neuron with correct weights could output results associated with simple logic gates (OR/NOT/AND).  However, they did not present a way for the neuron to learn the weights from exemplary data. 

In 1949, Donald Hebb wrote `The Organization of Behavior. <https://link.springer.com/chapter/10.1007/978-3-642-70911-1_15>`_  He made the point that neural pathways strengthen as they are used, and this is instrumental in how people learn.  He didn't propose a mathematical model for how this might happen, though Hebb's 'rule' inspired later work: 
   
   *When an axon of cell A is near enough to excite a cell B and repeatedly or persistently takes part in firing it, some growth process or metabolic change takes place in one or both cells such that A’s efficiency, as one of the cells firing B, is increased.*

In 1957 Frank Rosenblatt invented the Perceptron. The Perceptron had two major improvements over prior implementations of artificial neurons.  First, it had an adjustable threshold (a bias) for neuron activation.  Second, he proposed a mathematical formulation of Hebbs rule that allowed weights governing the connection between neurons to change, and thereby "learn" from training data.  Dr. Rosenblatt was optimistic of the perceptron's promise: 
   
   *The Navy revealed the embryo of an electronic computer today that it expects will be able to walk, talk, see, write, reproduce itself an be conscious of its existence … Dr. Frank Rosenblatt, a research psychologist at the Cornell Aeronautical Laboratory, Buffalo, said Perceptrons might be fired to the planets as mechanical space explorers* (`Source <http://www.nytimes.com/1958/07/08/archives/new-navy-device-learns-by-doing-psychologist-shows-embryo-of.html>`_)

In the 1960s `considerable hype <https://www.youtube.com/watch?v=aygSMgK3BEM>`_ was building around artificial intelligence and neural networks in particlar. But Marvin Minsky and Seymour Papert of the MIT AI Lab were skeptical and published a rigorous analysis on of the limitations of Perceptrons in a book named `Perceptrons. <https://en.wikipedia.org/wiki/Perceptrons_(book)>`_   In this book they showed that a Perceptron could not learn the non-linear separation ability necessary to predict the simple exclusive OR function (XOR) and therefore was fundamentally limited.  Though they mentioned that a network comprised of multiple layers of perceptrons could do this, there was no way to learn the weights in multi-layer networks at that time. This publication and other factors preceded the first `A.I. Winter. <https://en.wikipedia.org/wiki/AI_winter>`_

Work in the '70s and '80s helped lead to a thaw.  Recognizing that multiple layers of perceptrons (the multilayer perceptron, MLP) were needed to model complex non-linear behavior, research investigated ways to **backpropogate** the error back to layers deeper in the network so that those weights could be updated to help learn the function of interest.  Backpropogation - the recursive application of the chain rule to calculate the gradient of the error with respect to the weights of interest so that gradient descent could be used to update those weights in a neural network - was first proposed in the U.S. by Paul Werbos in his `1974 PhD Thesis. <https://www.bibsonomy.org/bibtex/2b0644d7aa84be0df0f198d586d341843/schaul>`_ However, it wasn't until 1986 when this approach was revisted by David Rumelhart, Geoffrey Hinton, and Ronald Williams in `Learning representations by back-propogating errors <https://www.iro.umontreal.ca/~vincentp/ift3395/lectures/backprop_old.pdf>`_ that interest was significantly renewed.  Multilayer perceptrons are a general purpose machine learning model and have been used to model such diverse topics as human driving `(ALVINN) <http://repository.cmu.edu/cgi/viewcontent.cgi?article=2874&context=compsci>`_, `time series prediction <https://pdfs.semanticscholar.org/82c8/e5d0cd4a7467f7f54ad823b2136b973eeb6e.pdf>`_, and `language modeling. <http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=5E92CE80772BE2E3F5E26AD0597BBEC9?doi=10.1.1.103.4626&rep=rep1&type=pdf>`_ 

Rumelhart et. al. published another paper in 1986, `Learning Internal Representations by Error Propogation, <http://www.cs.toronto.edu/~fritz/absps/pdp8.pdf>`_ that described a special type of multilayer perceptron network, the autoencoder. Autoencoders look for a way to encode information present in a higher dimensional space into a lower dimensional space, and then reconstruct information back into the higher dimensional space.   Autoencoders have obvious applications in dimensionaility reduction and compression, but can be used for `denoising images <https://arxiv.org/pdf/1608.04667.pdf>`_ and `reconstructing faces. <https://www.cc.gatech.edu/~hays/7476/projects/Avery_Wenchen/>`_  

In 1989 Yann Lecunn and others at Bell Labs showed that another form of neural network, a convolutional neural network (CNN), could `read zip codes. <http://yann.lecun.com/exdb/publis/pdf/lecun-89e.pdf>`_  There were several remarkable take-aways from their paper.  They didn't do any feature engineering to the images before presenting them as input to the network.  Their model architecture was inspired by work in visual pattern recognition that placed importance on the local relationship of features in the visual field and hierarchical relationships formed by combining simple features into more complex features in progressive levels.  In CNNs, local features extract simple information in preliminary layers, but complex information deeper into the network.  Local information is extracted using small visual filters (convolutional kernels) that scan across the images/maps, where the values in the kernels are learned by backpropogation and gradient descent.  One of the most famous CNNs is Visual Geometry Group's `VGG16 <https://arxiv.org/pdf/1409.1556.pdf>`_ entry into the ImageNet ILSVRC-2014 competition which one first and second place in the localization and classification tasks.

The network architectures discussed above are feed forward neural networks, meaning that information moves only one direction: from input to prediction (input layer -> hidden layers -> output layer) in a directed, acyclic (no loops) graph.  In a recurrent neural network (RNN), layers can send information back to previous layers in a cycle.  The reason for this is to allow inputs that have come before to influence the state of the network, and use this state to predict different things for a given input. For instance, in the case of speech prediction the words "Morning" and "Night" might both come after the word "Good," but prior context from the conversation could be used to narrow down the possibility of one or the other.  John Hopfield presented `Hopfield Networks <https://en.wikipedia.org/wiki/Hopfield_network>`_ in 1982, and recurrent neural nets are discussed by Michael Jordan in `Serial Order, A Parallel Distributed Processing Approach <http://cseweb.ucsd.edu/~gary/PAPER-SUGGESTIONS/Jordan-TR-8604.pdf>`_  in 1986.  In 1993 Yoshua Bengio wrote about the difficulty associated with training recurrent neural networks in `A Connectionist Approach to Speech Recognition. <http://www.iro.umontreal.ca/~lisa/publications2/index.php/attachments/single/161>`_  He stated that RNNs were able to recall short-term dependencies (e.g., remembering a few words back) but struggled with longer term dependencies (e.g. a sentence back).  The culprit, as stated by Sepp Hochreiter in his PhD Thesis in 1991 and restated in English in `this article <http://www.bioinf.jku.at/publications/older/2304.pdf>`_  was vanishing and exploding gradients.  RNNs tend to be deep as the network is unrolled for each entry in the sequence, and the gradients calculated from backpropogation tend to either vanish or explode which prevent the RNN from converging.  Hochreiter and Schmidhuber (his PhD advisor) authored a solution in 1997: `Long Short-Term Memory. <http://www.bioinf.jku.at/publications/older/2604.pdf>`_  They proposed clipping the gradient and introducing constant error flow as solutions to the exploding/vanishing gradient problems.  Around 2007 LSTMs started to revolutionize speech recognition and have broken records for machine translation and language modeling since.

These four architectures - MLP, Autoencoder, CNN, and RNN - are presented in this short course.  Other neural network architectures exist, and there are many variants of even these four architectures.

Professor Geoff Hinton, in this `lecture captured in 2016 <https://www.youtube.com/watch?time_continue=1329&v=bk7fM_EjKHo>`_ summarized what has held back neural networks in the numerous A.I. winters throughout the years. Here is his list:  

1. Our labeled data sets were thousands of times too small.  
2. Our computers were millions of times too slow.  
3. We initialized the weights in a stupid way.  
4. We used the wrong type of non-linearity.  

Points 3 and 4 will become clearer as you proceed with this course.

Andrey Kurenkov's `'Brief' History of Neural Nets and Deep Learning <http://www.andreykurenkov.com/writing/a-brief-history-of-neural-nets-and-deep-learning/>`_ was a valuable resource during for the preparation of this summary, as was Wikipedia.


Tensorflow
----------

From `Tensorflow's website: <https://www.tensorflow.org/>`_  

    TensorFlow is an open source software library for numerical computation using data flow graphs. Nodes in the graph represent mathematical operations, while the graph edges represent the multidimensional data arrays (tensors) communicated between them. The flexible architecture allows you to deploy computation to one or more CPUs or GPUs in a desktop, server, or mobile device with a single API. TensorFlow was originally developed by researchers and engineers working on the Google Brain Team within Google's Machine Intelligence research organization for the purposes of conducting machine learning and deep neural networks research, but the system is general enough to be applicable in a wide variety of other domains as well. 

Tensorflow was open-sourced in November 2015 and has a growing development community (https://www.tensorflow.org, https://github.com/jtoy/awesome-tensorflow). It has a Python API but converts operations to optimized C++ code (it also has a C++ API). Other high-level APIs have been built on top of Tensorflow, such as `Keras. <https://keras.io/>`_ It provides parameter optimization routines to minimize cost functions, and it automatically computes the gradients of functions you define using automatic differentiating (*autodiff*).  It comes with `Tensorboard <https://www.tensorflow.org/get_started/summaries_and_tensorboard>`_ - a visualization tool that lets you inspect the computation graph and the training process.  And though Tensorflow is relatively young, trends on Stackoverflow show that its use (as measured by number of queries) is `growing. <https://insights.stackoverflow.com/trends?tags=tensorflow%2Ccaffe>`_
