{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 4b: Fully connected autoencoder on MNIST dataset\n",
    "\n",
    "Your challenge, if you should accept it, is to tune a convolutional autoencoder on the [MNIST dataset](https://en.wikipedia.org/wiki/MNIST_database). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Autoencoder\n",
    "\n",
    "<img src=\"https://cdn-images-1.medium.com/max/1600/1*MxWMSjR0BZzb7bnVGJpdng.png\" alt=\"nn\" style=\"width: 400px;\"/>\n",
    "\n",
    "## MNIST Dataset Review\n",
    "\n",
    "This example is using MNIST handwritten digits. The dataset contains 60,000 examples for training and 10,000 examples for testing. The digits have been size-normalized and centered in a fixed-size image (28x28 pixels) with values from 0 to 1. For simplicity, each image has been flattened and converted to a 1-D numpy array of 784 features (28*28).\n",
    "\n",
    "![MNIST Dataset](http://neuralnetworksanddeeplearning.com/images/mnist_100_digits.png)\n",
    "\n",
    "More info: http://yann.lecun.com/exdb/mnist/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf \n",
    "import matplotlib.pyplot as plt\n",
    "import gzip, binascii, struct\n",
    "from IPython.display import Image\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import, scale, transform data\n",
    "\n",
    "In the next few cells, we will be downloading the mnist data, scaling the images so that pixel values are between [0, 1], and transforming images to numpy arrays. Note that functions for extracting and transforming labels are present, but not called - we will not use labels in this activity. These initial sections are repeated from previous exercises and so will be done without further comment.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the training set:  (55000, 784)\n",
      "Shape of the test set:  (10000, 784)\n"
     ]
    }
   ],
   "source": [
    "train_images = mnist.train.images\n",
    "test_images = mnist.test.images\n",
    "print(\"Shape of the training set: \", train_images.shape)\n",
    "print(\"Shape of the test set: \", test_images.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reshape data into 2D images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = train_images.reshape(-1, 28, 28, 1)\n",
    "test_images = test_images.reshape(-1, 28, 28, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Segmenting data into training, test, and validation\n",
    "\n",
    "The final step in preparing our data is to split it into three sets: training, test, and validation. This isn't the format of the original data set, so we'll take a small slice of the training data and treat that as our validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation shape (5000, 28, 28, 1)\n",
      "Train size 50000\n"
     ]
    }
   ],
   "source": [
    "VALIDATION_SIZE = 5000\n",
    "\n",
    "# first, let's split the flattened data - we will skip doing the labels, since we are doing an autoencoder\n",
    "validation_images = train_images[:VALIDATION_SIZE, :, :, :]\n",
    "train_images = train_images[VALIDATION_SIZE:, :, :, :]\n",
    "\n",
    "train_size = train_images.shape[0]\n",
    "\n",
    "print('Validation shape', validation_images.shape)\n",
    "print('Train size', train_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the model\n",
    "\n",
    "Now that we've downloaded and prepared our data, we're ready to define our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Hyperparameters\n",
    "\n",
    "First, let's define some hyperparameters. You may want to change these later to try for better results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this gives us the size of input and output data - you shouldn't change this\n",
    "IMAGE_SIZE = train_images.shape[1]\n",
    "CHANNELS = train_images.shape[-1]\n",
    "\n",
    "# Tunable hyperparameters - tweak away!\n",
    "BATCH_SIZE = 60\n",
    "\n",
    "# The random seed that defines initialization.\n",
    "SEED = 42\n",
    "\n",
    "# Optimizer hyperparameters, can be tuned\n",
    "LEARNING_RATE = 0.002\n",
    "BETA1 = 0.9\n",
    "BETA2 = 0.999\n",
    "EPSILON = 1e-08\n",
    "\n",
    "# Define the optimizer operation - can also pick a different optimizer \n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=LEARNING_RATE,\n",
    "                                  beta1=BETA1,\n",
    "                                  beta2=BETA2,\n",
    "                                  epsilon=EPSILON,\n",
    "                                  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define placeholder tensor to feed data\n",
    "\n",
    "Normally, we would also define a placeholder tensor for y. Since we are doing an autoencoder, we are training X vs. X, so we only need an X placeholder!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, shape=(None, IMAGE_SIZE, IMAGE_SIZE, CHANNELS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining weights for the fully connected autoencoder\n",
    "\n",
    "Note that this autoencoder is using fully connected layers, so the weights will be two dimensional matrices. \n",
    "\n",
    "The tunable hyperparameters are not the only thing that affects your model....your architecture and initialization of weights also. Try changing weight initializations, number of filters, or adding more layers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# encoder portion where dimensionality decreases\n",
    "enc1_weights = tf.Variable(\n",
    "  tf.truncated_normal([3, 3, CHANNELS, 32],  # 32 filters\n",
    "                      stddev=0.1,\n",
    "                      seed=SEED))\n",
    "enc1_biases = tf.Variable(tf.constant(0.1, shape=[32]))\n",
    "\n",
    "enc2_weights = tf.Variable(\n",
    "  tf.truncated_normal([3, 3, 32, 32], # 64 filters\n",
    "                      stddev=0.1,\n",
    "                      seed=SEED))\n",
    "enc2_biases = tf.Variable(tf.constant(0.1, shape=[32]))\n",
    "\n",
    "# decoder portion where dimensionality increases\n",
    "dec1_weights = tf.Variable(  \n",
    "  tf.truncated_normal([3, 3, 32, 32],\n",
    "                      stddev=0.1,\n",
    "                      seed=SEED))\n",
    "dec1_biases = tf.Variable(tf.constant(0.1, shape=[32]))\n",
    "\n",
    "upsample1_weights = tf.Variable(\n",
    "  tf.truncated_normal([3, 3, 32, 32],\n",
    "                      stddev=0.1,\n",
    "                      seed=SEED))\n",
    "\n",
    "upsample2_weights = tf.Variable(\n",
    "  tf.truncated_normal([3, 3, 32, 32],\n",
    "                      stddev=0.1,\n",
    "                      seed=SEED))\n",
    "\n",
    "dec2_weights = tf.Variable(\n",
    "  tf.truncated_normal([3, 3, 32, CHANNELS],\n",
    "                      stddev=0.1,\n",
    "                      seed=SEED))\n",
    "dec2_biases = tf.Variable(tf.constant(0.1, shape=[CHANNELS]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_encoder(data, train=False):\n",
    "    \"\"\"The convolutional encoder model portion.\"\"\"\n",
    "    # first encoding layer\n",
    "    conv_enc1 = tf.nn.conv2d(data,\n",
    "                            enc1_weights,\n",
    "                            strides=[1, 1, 1, 1],\n",
    "                            padding='SAME')\n",
    "    # Now 28x28x32\n",
    "\n",
    "    # Bias and rectified linear non-linearity.\n",
    "    relu1 = tf.nn.relu(tf.nn.bias_add(conv_enc1, enc1_biases)) \n",
    "\n",
    "    # Max pooling. The kernel size spec ksize also follows the layout of\n",
    "    # the data. Here we have a pooling window of 2, and a stride of 2.\n",
    "    pool1 = tf.nn.max_pool(relu1,\n",
    "                          ksize=[1, 2, 2, 1],\n",
    "                          strides=[1, 2, 2, 1],\n",
    "                          padding='SAME')\n",
    "    # Now 14x14x32\n",
    "    \n",
    "    # second encoding layer\n",
    "    conv_enc2 = tf.nn.conv2d(pool1,\n",
    "                            enc2_weights,\n",
    "                            strides=[1, 1, 1, 1],\n",
    "                            padding='SAME')\n",
    "    # Now 14x14x32\n",
    "\n",
    "    # Bias and rectified linear non-linearity.\n",
    "    relu2 = tf.nn.relu(tf.nn.bias_add(conv_enc2, enc2_biases)) \n",
    "\n",
    "    # Max pooling. The kernel size spec ksize also follows the layout of\n",
    "    # the data. Here we have a pooling window of 2, and a stride of 2.\n",
    "    pool2 = tf.nn.max_pool(relu2,\n",
    "                          ksize=[1, 2, 2, 1],\n",
    "                          strides=[1, 2, 2, 1],\n",
    "                          padding='SAME')\n",
    "    # Now 7x7x32    \n",
    "    return pool2\n",
    "\n",
    "\n",
    "def conv_decoder(encoded, train=False):\n",
    "    \"\"\"The convolutional decoder model portion.\"\"\"\n",
    "    conv_dec1 = tf.nn.conv2d(encoded,\n",
    "                            dec1_weights,\n",
    "                            strides = [1, 1, 1, 1],\n",
    "                            padding = 'SAME')\n",
    "    #Now 7x7x32 \n",
    "    \n",
    "    relu1 = tf.nn.relu(tf.nn.bias_add(conv_dec1, dec1_biases))\n",
    "    \n",
    "    # get shape of inputs b/c tensorflow doesn't like \"None\"\n",
    "    input_size = tf.shape(encoded)[0]\n",
    "    \n",
    "    upsample1 = tf.nn.conv2d_transpose(relu1,\n",
    "                                      upsample1_weights,\n",
    "                                      output_shape = [input_size, 14, 14, 32],\n",
    "                                      strides = [1, 2, 2, 1],\n",
    "                                      padding = 'SAME')\n",
    "    # Now 14x14x32\n",
    "    \n",
    "    upsample2 = tf.nn.conv2d_transpose(upsample1,\n",
    "                                      upsample2_weights,\n",
    "                                      output_shape = [input_size, 28, 28, 32],\n",
    "                                      strides = [1, 2, 2, 1],\n",
    "                                      padding = 'SAME')\n",
    "    # Now 28x28x32\n",
    "    \n",
    "    conv_dec2 = tf.nn.conv2d(upsample2,\n",
    "                            dec2_weights,\n",
    "                            strides = [1, 1, 1, 1],\n",
    "                            padding = 'SAME')\n",
    "    #Now 28x28x1\n",
    "    \n",
    "    # using a sigmoid because it forces values to be between 0 and 1...but this might not be the best choice.\n",
    "    recon = tf.nn.sigmoid(tf.nn.bias_add(conv_dec2, dec2_biases))    \n",
    "    return recon\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define loss and training operations. We are comparing the reconstructed output to the input.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded = conv_encoder(X)\n",
    "decoded = conv_decoder(encoded)\n",
    "\n",
    "loss_op = tf.reduce_mean((X - decoded)**2) # pixel-wise MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_op = optimizer.minimize(loss_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize the variables (i.e. assign their default value)\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code will plot your test images vs. the reconstructed images from the autoencoder. This will help you visualize how well your autoencoder is doing! We will call this function in the train block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_reconstruction(X_orig, X_decoded, n = 10, plotname = None):\n",
    "    '''\n",
    "    inputs: X_orig (2D np array of shape (nrows, 784))\n",
    "            X_recon (2D np array of shape (nrows, 784))\n",
    "            n (int, number of images to plot)\n",
    "            plotname (str, path to save file)\n",
    "    '''\n",
    "    fig = plt.figure(figsize=(n*2, 4))\n",
    "    for i in range(n):\n",
    "        # display original\n",
    "        ax = fig.add_subplot(2, n, i + 1)\n",
    "        plt.imshow(X_orig[i].reshape(28, 28))\n",
    "        plt.gray()\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "\n",
    "        # display reconstruction\n",
    "        ax = fig.add_subplot(2, n, i + 1 + n)\n",
    "        plt.imshow(X_decoded[i].reshape(28, 28))\n",
    "        plt.gray()\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "        \n",
    "        fig.suptitle('Reconstructed inputs')\n",
    "\n",
    "    if plotname:\n",
    "        plt.savefig(plotname)\n",
    "    else:\n",
    "        plt.show()       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, a function to train your autoencoder... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(num_epochs): \n",
    "    \"\"\"Function that trains model \"\"\"\n",
    "    train_size = train_images.shape[0]\n",
    "    steps = num_epochs * train_size // BATCH_SIZE\n",
    "    steps_per_epoch = train_size // BATCH_SIZE\n",
    "          \n",
    "    with tf.Session() as sess:\n",
    "\n",
    "        # Run the initializer\n",
    "        sess.run(init)    \n",
    "          \n",
    "        for step in range(steps):\n",
    "            # Compute the offset of the current minibatch in the data.\n",
    "            # Note that we could use better randomization across epochs.\n",
    "            offset = (step * BATCH_SIZE) % (train_size - BATCH_SIZE)\n",
    "            batch_data = train_images[offset:(offset + BATCH_SIZE), :, :, :]\n",
    "          \n",
    "            # Run the training operation to update the weights, use a feed_dict to use the batches you created above\n",
    "            sess.run(train_op, feed_dict={X: batch_data})\n",
    "        \n",
    "            # display output if desired\n",
    "            if step % steps_per_epoch == 0:\n",
    "                loss = sess.run(loss_op, feed_dict={X: batch_data})\n",
    "                print(\"Epoch {}, Minibatch MSE= {:.3f}\".format(str(step//steps_per_epoch), loss))\n",
    "            \n",
    "                val_loss = sess.run(loss_op, feed_dict = {X: validation_images})\n",
    "                print(\"Validation MSE : {}\".format(val_loss))\n",
    "\n",
    "        print(\"Optimization Finished!\")\n",
    "          \n",
    "        # Run the accuracy operations for the CIFAR10 test images\n",
    "        test_decoded, test_loss = sess.run([decoded, loss_op], feed_dict={X: test_images})\n",
    "        print(\"Test MSE : {}\".format(test_loss))\n",
    "        plot_reconstruction(test_images, test_decoded) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, train your autoencoder. \n",
    "\n",
    "With the given inputs, you will most likely get stuck in a local minimum. Happy tuning!\n",
    "\n",
    "Warning....this one takes longer to train than the fully connected autoencoder! You may want to subset the data to make this go a little faster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
